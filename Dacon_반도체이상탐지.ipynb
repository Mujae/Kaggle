{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!mkdir DATA\n",
        "!unzip -qq {'/content/drive/MyDrive/데이콘/반도체/open.zip'} -d /content/DATA"
      ],
      "metadata": {
        "id": "zfhmNqsBF6SD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
      ],
      "metadata": {
        "id": "FLKKcN0oBl5T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X9xNwm_cBDyU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from tqdm import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 사용 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "G0ujkbOmBLcL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(176) # Seed 고정"
      ],
      "metadata": {
        "id": "OD50-5fFBN6R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로딩 클래스 정의\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): csv 파일의 경로.\n",
        "            transform (callable, optional): 샘플에 적용될 Optional transform.\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.df['img_path'].iloc[idx]\n",
        "        image = Image.open('/content/DATA'+img_path[1:])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        target = torch.tensor([0.]).float()\n",
        "        return image, target\n",
        "\n",
        "# 이미지 전처리 및 임베딩\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_data = CustomDataset(csv_file='/content/DATA/train.csv', transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "R8QlKYEiBPXC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = nn.Linear(in_features=512, out_features=1, bias=True)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0.00001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzRO95zcLLjv",
        "outputId": "b2c2c810-063e-4c09-feea-3a6e3170e298"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 152MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        print('start_',epoch)\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels.view(-1, 1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            running_corrects += torch.sum(predictions == labels.view(-1, 1)).item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_acc = running_corrects / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n"
      ],
      "metadata": {
        "id": "P52PH-MRL7KU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 실행\n",
        "train(model, train_loader, criterion, optimizer, scheduler, num_epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XgNSfYmMKD8",
        "outputId": "2bfe83f5-6fbe-4b76-ccd5-9843403b6b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_ 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 학습된 모델 로드\n",
        "model.eval()  # 추론 모드로 설정\n",
        "\n",
        "# 특성 추출을 위한 모델의 마지막 레이어 수정\n",
        "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# 이미지를 임베딩 벡터로 변환\n",
        "def get_embeddings(dataloader, model):\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in tqdm(dataloader):\n",
        "            images = images.to(device)\n",
        "            emb = model(images)\n",
        "            embeddings.append(emb.cpu().numpy().squeeze())\n",
        "    return np.concatenate(embeddings, axis=0)\n",
        "\n",
        "train_embeddings = get_embeddings(train_loader, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmbp6cjcNpY9",
        "outputId": "b5976b52-5703-4526-b54c-c80ccb941ac2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:02<00:00,  3.30it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1-rZAHON0kA",
        "outputId": "4bc38caf-e2e4-45e1-a7a9-5e61d9e2644c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(213, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# type: ignore\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "def l2_distance(z):\n",
        "    diff = tf.expand_dims(z, axis=1) - tf.expand_dims(z, axis=0)\n",
        "    return tf.reduce_sum(diff ** 2, axis=-1)\n",
        "\n",
        "\n",
        "class PairwiseSimilarity(layers.Layer):\n",
        "    def __init__(self, sigma=1.0):\n",
        "        super(PairwiseSimilarity, self).__init__()\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def call(self, z):\n",
        "        return tf.exp(-l2_distance(z) / self.sigma)\n",
        "\n",
        "\n",
        "class ContextualSimilarity(layers.Layer):\n",
        "    def __init__(self, k):\n",
        "        super(ContextualSimilarity, self).__init__()\n",
        "        self.k = k\n",
        "\n",
        "    def call(self, z):\n",
        "        distances = l2_distance(z)\n",
        "        kth_nearst = -tf.math.top_k(-distances, k=self.k, sorted=True)[0][:, -1]\n",
        "        mask = tf.cast(distances <= tf.expand_dims(kth_nearst, axis=-1), tf.float32)\n",
        "\n",
        "        similarity = tf.matmul(mask, mask, transpose_b=True) / tf.reduce_sum(mask, axis=-1, keepdims=True)\n",
        "        R = mask * tf.transpose(mask)\n",
        "        similarity = tf.matmul(similarity, R, transpose_b=True) / tf.reduce_sum(R, axis=-1, keepdims=True)\n",
        "        return 0.5 * (similarity + tf.transpose(similarity))\n",
        "\n",
        "\n",
        "class ReConPatch(keras.Model):\n",
        "    def __init__(self, input_dim, embedding_dim, projection_dim, alpha, margin=0.1):\n",
        "        super(ReConPatch, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.margin = margin\n",
        "\n",
        "        # embedding & projection layers\n",
        "        self.embedding = layers.Dense(embedding_dim)\n",
        "        self.projection = layers.Dense(projection_dim)\n",
        "\n",
        "        # ema ver of embedding & projection layers\n",
        "        self.ema_embedding = layers.Dense(embedding_dim, trainable=False)\n",
        "        self.ema_projection = layers.Dense(projection_dim, trainable=False)\n",
        "\n",
        "        # initialize layers\n",
        "        self.embedding.build((None, input_dim))\n",
        "        self.projection.build((None, embedding_dim))\n",
        "        self.ema_embedding.build((None, input_dim))\n",
        "        self.ema_projection.build((None, embedding_dim))\n",
        "\n",
        "        # ema operator\n",
        "        self.ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
        "        self.update_ema()\n",
        "\n",
        "        self.pairwise_similarity = PairwiseSimilarity(sigma=1.0)\n",
        "        self.contextual_similarity = ContextualSimilarity(k=3)\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.embedding(x)\n",
        "\n",
        "    def train_step(self, x):\n",
        "        h_ema = self.ema_embedding(x)\n",
        "        z_ema = self.ema_projection(h_ema)\n",
        "\n",
        "        p_sim = self.pairwise_similarity(z_ema)\n",
        "        c_sim = self.contextual_similarity(z_ema)\n",
        "        w = self.alpha * p_sim + (1 - self.alpha) * c_sim\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            h = self.embedding(x)\n",
        "            z = self.projection(h)\n",
        "\n",
        "            # Contrastive loss\n",
        "            distances = tf.sqrt(l2_distance(z) + 1e-9)\n",
        "            delta = distances / tf.reduce_mean(distances, axis=-1, keepdims=True)\n",
        "            rc_loss = tf.reduce_sum(tf.reduce_mean(\n",
        "                w * (delta ** 2) + (1 - w) * (tf.nn.relu(self.margin - delta) ** 2),\n",
        "                axis=-1\n",
        "            ))\n",
        "        # Update weights\n",
        "        self.optimizer.minimize(rc_loss, self.trainable_variables, tape=tape)\n",
        "        # Update EMA\n",
        "        self.update_ema()\n",
        "\n",
        "        return {\"rc_loss\": rc_loss}\n",
        "\n",
        "    def update_ema(self):\n",
        "        self.ema.apply(self.embedding.weights + self.projection.weights)\n",
        "\n",
        "        avg_emb_w, avg_emb_b = self.ema.average(self.embedding.weights[0]), self.ema.average(self.embedding.weights[1])\n",
        "        avg_proj_w, avg_proj_b = self.ema.average(self.projection.weights[0]), self.ema.average(self.projection.weights[1])\n",
        "\n",
        "        self.ema_embedding.set_weights([avg_emb_w, avg_emb_b])\n",
        "        self.ema_projection.set_weights([avg_proj_w, avg_proj_b])"
      ],
      "metadata": {
        "id": "mLIjlHAbCZy_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "# Isolation Forest 모델 학습\n",
        "clf = IsolationForest(random_state=42)\n",
        "clf.fit(train_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "sfwipMlTQviE",
        "outputId": "774ddce3-638d-4c6b-9130-e3aad7763e13"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IsolationForest(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터에 대해 이상 탐지 수행\n",
        "test_data = CustomDataset(csv_file='/content/DATA/test.csv', transform=transform)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "test_embeddings = get_embeddings(test_loader, model)\n",
        "test_pred = clf.predict(test_embeddings)\n",
        "\n",
        "# Isolation Forest의 예측 결과(이상 = -1, 정상 = 1)를 이상 = 1, 정상 = 0으로 변환\n",
        "test_pred = np.where(test_pred == -1, 1, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xtquTS1QwXZ",
        "outputId": "c342329c-e280-4f22-8ed4-16218b97d0fd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:01<00:00,  3.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('/content/DATA/sample_submission.csv')\n",
        "submit['label'] = test_pred\n",
        "submit.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "vLS2CAPfQ2nL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recon"
      ],
      "metadata": {
        "id": "iTPyEaqAQtVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReConPatch 모델 인스턴스화\n",
        "model = ReConPatch(input_dim=512, embedding_dim=512, projection_dim=256, alpha=1, margin=0.1)\n",
        "model.compile(optimizer='adam')\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(train_dataset, epochs=10)\n",
        "\n",
        "# 모델 평가 (여기서는 학습 데이터로 간단히 평가)\n",
        "# 실제 사용 시 별도의 검증 데이터셋을 준비해야 함\n",
        "print(\"모델 평가:\")\n",
        "model.evaluate(train_dataset)\n"
      ],
      "metadata": {
        "id": "N_Th4mRmPgSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 데이터 준비: 여기서는 예제를 위한 임시 데이터를 생성합니다.\n",
        "# 실제 사용 시에는 적절한 입력 데이터를 준비해야 합니다.\n",
        "input_dim =  # 예제 입력 차원\n",
        "embedding_dim = 512  # 임베딩 차원\n",
        "projection_dim = 32  # 프로젝션 차원\n",
        "\n",
        "x_train = np.random.random((num_samples, input_dim)).astype(np.float32)\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "model = ReConPatch(input_dim, embedding_dim, projection_dim, alpha=0.5)\n",
        "\n",
        "# 컴파일러 설정: 옵티마이저, 손실 함수 등\n",
        "model.compile(optimizer=Adam(learning_rate=1e-3))\n",
        "\n",
        "# 학습 과정: x_train을 입력과 목표 모두로 사용합니다.\n",
        "# 이 모델은 자기 자신을 학습하는 self-supervised 모델의 특성을 가집니다.\n",
        "model.fit(x_train, x_train, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "id": "aSAS4Zd_Ga9g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}