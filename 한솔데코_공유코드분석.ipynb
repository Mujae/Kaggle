{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "85636527-d0a6-4d46-9481-8f9f5d9c657b",
      "metadata": {
        "id": "85636527-d0a6-4d46-9481-8f9f5d9c657b"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146d3550-a0d9-4602-9c09-222953286b66",
      "metadata": {
        "id": "146d3550-a0d9-4602-9c09-222953286b66"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "import bitsandbytes as bnb\n",
        "import os\n",
        "import wandb\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast, AdamW, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from tqdm import tqdm\n",
        "\n",
        "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "#torch.backends.cuda.matmul.allow_tf32=True\n",
        "#torch.set_float32_matmul_precision('medium')\n",
        "#torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19ec3b9c-5bd3-48e7-bb73-32469f77bdfd",
      "metadata": {
        "id": "19ec3b9c-5bd3-48e7-bb73-32469f77bdfd"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cadc43a-1acf-461e-b19b-44314861a27f",
      "metadata": {
        "id": "5cadc43a-1acf-461e-b19b-44314861a27f"
      },
      "outputs": [],
      "source": [
        "# 데이터 로드\n",
        "data = pd.read_csv('train.csv')\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained('LDCC/LDCC-SOLAR-10.7B',  eos_token='</s>')#롯데통신이 있었구만 ㅋㅋ;\n",
        "\n",
        "max_length = 128\n",
        "\n",
        "formatted_data = []\n",
        "for _, row in tqdm(data.iterrows()):\n",
        "  for q_col in ['질문_1', '질문_2']:\n",
        "    for a_col in ['답변_1', '답변_2', '답변_3', '답변_4', '답변_5']:\n",
        "      input_text = row[q_col] + tokenizer.eos_token + row[a_col]\n",
        "      input_ids = tokenizer.encode(input_text, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length)\n",
        "      formatted_data.append(input_ids)\n",
        "print('Done.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5da0bff",
      "metadata": {
        "id": "e5da0bff"
      },
      "outputs": [],
      "source": [
        "formatted_data = torch.cat(formatted_data, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4105cb04-bc9c-4bb5-8230-cbd84f34d4fb",
      "metadata": {
        "id": "4105cb04-bc9c-4bb5-8230-cbd84f34d4fb"
      },
      "source": [
        "## Model Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11cf4940-6155-43ac-9315-2a17e56a06b1",
      "metadata": {
        "id": "11cf4940-6155-43ac-9315-2a17e56a06b1"
      },
      "outputs": [],
      "source": [
        "# 모델 로드\n",
        "\n",
        "model_id = \"LDCC/LDCC-SOLAR-10.7B\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             device_map={\"\":0},\n",
        "                                             #torch_dtype=torch.float32,\n",
        "\n",
        "                                             )\n",
        "\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "\n",
        "config = LoraConfig(#PEFT\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    #target_modules=[\"query_key_value\"],\n",
        "    target_modules=[\n",
        "    \"q_proj\",\n",
        "    \"up_proj\",\n",
        "    \"o_proj\",\n",
        "    \"k_proj\",\n",
        "    \"down_proj\",\n",
        "    \"gate_proj\",\n",
        "    \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=formatted_data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "      #  max_steps=50,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}